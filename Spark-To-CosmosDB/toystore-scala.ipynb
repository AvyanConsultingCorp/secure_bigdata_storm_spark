{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "// Import Spark to Cosmos DB Connector\nimport com.microsoft.azure.cosmosdb.spark.schema._\nimport com.microsoft.azure.cosmosdb.spark._\nimport com.microsoft.azure.cosmosdb.spark.config.Config", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>11</td><td>application_1521344906017_0015</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-toyfac.pqi3astrdrvernbvkgbxedj5ih.cx.internal.cloudapp.net:8088/proxy/application_1521344906017_0015/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn1-toyfac.pqi3astrdrvernbvkgbxedj5ih.cx.internal.cloudapp.net:30060/node/containerlogs/container_1521344906017_0015_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}, {"output_type": "stream", "name": "stderr", "text": "<console>:24: error: object cosmosdb is not a member of package com.microsoft.azure\n       import com.microsoft.azure.cosmosdb.spark.schema._\n                                  ^\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "%%configure\n{ \"name\":\"Spark-to-Cosmos_DB_Connector\", \n  \"executorMemory\": \"1G\", \n  \"executorCores\": 2, \n  \"numExecutors\": 1,\n  \"driverMemory\" : \"1G\",\n  \"jars\": [\"wasb:///example/jars/azure-cosmosdb-spark_2.1.0_2.11-0.0.4.jar\", \"wasb:///example/jars/azure-documentdb-1.13.0.jar\", \"wasb:///example/jars/azure-documentdb-rx-0.9.0-rc2.jar\", \"wasb:///example/jars/json-20140107.jar\", \"wasb:///example/jars/rxjava-1.3.0.jar\", \"wasb:///example/jars/rxnetty-0.4.20.jar\"],  \n  \"conf\": {\n    \"spark.jars.packages\": \"graphframes:graphframes:0.5.0-spark2.1-s_2.11\",   \n    \"spark.jars.excludes\": \"org.scala-lang:scala-reflect\"\n   }\n}", "outputs": [{"output_type": "stream", "name": "stderr", "text": "A session has already been started. If you intend to recreate the session with new configurations, please include the -f argument.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure\n{ \"name\":\"Spark-to-Cosmos_DB_Connector\", \n  \"executorMemory\": \"1G\", \n  \"executorCores\": 2, \n  \"numExecutors\": 1,\n  \"driverMemory\" : \"1G\",\n  \"jars\": [\"wasb:///example/jars/azure-cosmosdb-spark_2.1.0_2.11-0.0.4.jar\", \"wasb:///example/jars/azure-documentdb-1.13.0.jar\", \"wasb:///example/jars/azure-documentdb-rx-0.9.0-rc2.jar\", \"wasb:///example/jars/json-20140107.jar\", \"wasb:///example/jars/rxjava-1.3.0.jar\", \"wasb:///example/jars/rxnetty-0.4.20.jar\"],  \n  \"conf\": {\n    \"spark.jars.packages\": \"graphframes:graphframes:0.5.0-spark2.1-s_2.11\",   \n    \"spark.jars.excludes\": \"org.scala-lang:scala-reflect\"\n   }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'name': u'Spark-to-Cosmos_DB_Connector', u'driverMemory': u'1G', u'numExecutors': 1, u'conf': {u'spark.jars.packages': u'graphframes:graphframes:0.5.0-spark2.1-s_2.11', u'spark.jars.excludes': u'org.scala-lang:scala-reflect'}, u'executorCores': 2, u'jars': [u'wasb:///example/jars/azure-cosmosdb-spark_2.1.0_2.11-0.0.4.jar', u'wasb:///example/jars/azure-documentdb-1.13.0.jar', u'wasb:///example/jars/azure-documentdb-rx-0.9.0-rc2.jar', u'wasb:///example/jars/json-20140107.jar', u'wasb:///example/jars/rxjava-1.3.0.jar', u'wasb:///example/jars/rxnetty-0.4.20.jar'], u'executorMemory': u'1G'}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "// Import Spark to Cosmos DB Connector\nimport com.microsoft.azure.cosmosdb.spark.schema._\nimport com.microsoft.azure.cosmosdb.spark._\nimport com.microsoft.azure.cosmosdb.spark.config.Config", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>12</td><td>application_1521344906017_0016</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-toyfac.pqi3astrdrvernbvkgbxedj5ih.cx.internal.cloudapp.net:8088/proxy/application_1521344906017_0016/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn1-toyfac.pqi3astrdrvernbvkgbxedj5ih.cx.internal.cloudapp.net:30060/node/containerlogs/container_1521344906017_0016_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nimport com.microsoft.azure.cosmosdb.spark.config.Config"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "// Connect to Cosmos DB Database\nval readConfig2 = Config(Map(\"Endpoint\" -> \"https://toyfactory.documents.azure.com:443/\",\n\"Masterkey\" -> \"uDp9gXJWbkucGznEkJBewJuiHhyJ4V1uLqle6DbsJ72egEJFL5e6MQUXzQGBZFjelJkmJPB0mFQn0haJbSlBLQ==\",\n\"Database\" -> \"toystore\",\n\"preferredRegions\" -> \"Central US\",\n\"Collection\" -> \"orders\", \n\"SamplingRatio\" -> \"1.0\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "readConfig2: com.microsoft.azure.cosmosdb.spark.config.Config = com.microsoft.azure.cosmosdb.spark.config.ConfigBuilder$$anon$1@4d7c77f5"}], "metadata": {"collapsed": false}}, {"execution_count": 15, "cell_type": "code", "source": "// Create collection connection \nval coll = spark.sqlContext.read.cosmosDB(readConfig2)\ncoll.createOrReplaceTempView(\"c\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 16, "cell_type": "code", "source": "%%sql\nselect * from c", "outputs": [{"ename": "AttributeError", "evalue": "'module' object has no attribute 'api'", "traceback": ["\u001b[1;31m\u001b[0m", "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/IPython/core/formatters.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    902\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36mdisplay_dataframe\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mselected_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mselected_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     encoding = Encoding(chart_type=Encoding.chart_type_table, x=selected_x, y=selected_y,\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36mselect_x\u001b[1;34m(data, order)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0m_validate_custom_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_classify_data_by_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mchosen_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36m_classify_data_by_type\u001b[1;34m(data, order, skip)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mskip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfer_vegalite_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/bin/anaconda/lib/python2.7/site-packages/autovizwidget/widget/utils.pyc\u001b[0m in \u001b[0;36minfer_vegalite_type\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \"\"\"\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     if typ in ['floating', 'mixed-integer-float', 'integer',\n", "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'api'"], "output_type": "error"}, {"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "                                    _etag  refund                      _rid  \\\n0  \"01003f70-0000-0000-0000-5aadf8b10000\"       0  NdhRANjYUwAIAAAAAAAAAA==   \n1  \"01004070-0000-0000-0000-5aadf8be0000\"       0  NdhRANjYUwAJAAAAAAAAAA==   \n2  \"01004670-0000-0000-0000-5aadf8c90000\"      20  NdhRANjYUwAKAAAAAAAAAA==   \n3  \"01004d70-0000-0000-0000-5aadf8f90000\"       0  NdhRANjYUwALAAAAAAAAAA==   \n4  \"01008375-0000-0000-0000-5aae1ab10000\"       0  NdhRANjYUwAMAAAAAAAAAA==   \n\n   _attachments  sales                                    id  \\\n0  attachments/     10  32a65f38-68e9-db59-5064-33b05bbb7513   \n1  attachments/     20  70bd78fa-6b5c-783e-7920-89926090bbe4   \n2  attachments/      0  0c760b9a-21fe-d93b-3611-ebbdd42a8108   \n3  attachments/     10  c8a08f6b-2da9-47d2-8d94-a0077e9b0b11   \n4  attachments/     50  9a195ce0-394e-67c8-02c4-cab0d86cd3b9   \n\n                                               _self  productid         _ts  \n0  dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...          1  1521350833  \n1  dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...          1  1521350846  \n2  dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...          1  1521350857  \n3  dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...          2  1521350905  \n4  dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...          2  1521359537  ", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_etag</th>\n      <th>refund</th>\n      <th>_rid</th>\n      <th>_attachments</th>\n      <th>sales</th>\n      <th>id</th>\n      <th>_self</th>\n      <th>productid</th>\n      <th>_ts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"01003f70-0000-0000-0000-5aadf8b10000\"</td>\n      <td>0</td>\n      <td>NdhRANjYUwAIAAAAAAAAAA==</td>\n      <td>attachments/</td>\n      <td>10</td>\n      <td>32a65f38-68e9-db59-5064-33b05bbb7513</td>\n      <td>dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...</td>\n      <td>1</td>\n      <td>1521350833</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"01004070-0000-0000-0000-5aadf8be0000\"</td>\n      <td>0</td>\n      <td>NdhRANjYUwAJAAAAAAAAAA==</td>\n      <td>attachments/</td>\n      <td>20</td>\n      <td>70bd78fa-6b5c-783e-7920-89926090bbe4</td>\n      <td>dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...</td>\n      <td>1</td>\n      <td>1521350846</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"01004670-0000-0000-0000-5aadf8c90000\"</td>\n      <td>20</td>\n      <td>NdhRANjYUwAKAAAAAAAAAA==</td>\n      <td>attachments/</td>\n      <td>0</td>\n      <td>0c760b9a-21fe-d93b-3611-ebbdd42a8108</td>\n      <td>dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...</td>\n      <td>1</td>\n      <td>1521350857</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"01004d70-0000-0000-0000-5aadf8f90000\"</td>\n      <td>0</td>\n      <td>NdhRANjYUwALAAAAAAAAAA==</td>\n      <td>attachments/</td>\n      <td>10</td>\n      <td>c8a08f6b-2da9-47d2-8d94-a0077e9b0b11</td>\n      <td>dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...</td>\n      <td>2</td>\n      <td>1521350905</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"01008375-0000-0000-0000-5aae1ab10000\"</td>\n      <td>0</td>\n      <td>NdhRANjYUwAMAAAAAAAAAA==</td>\n      <td>attachments/</td>\n      <td>50</td>\n      <td>9a195ce0-394e-67c8-02c4-cab0d86cd3b9</td>\n      <td>dbs/NdhRAA==/colls/NdhRANjYUwA=/docs/NdhRANjYU...</td>\n      <td>2</td>\n      <td>1521359537</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "products_agg = spark.sql(\"select productid,sum(sales) AS totalsales ,sum(refund) as totalrefund,(sum(sales)-sum(refund)) As profit from products group by productid\")", "outputs": [{"output_type": "stream", "name": "stderr", "text": "<console>:32: error: not found: value products_agg\nval $ires6 = products_agg\n             ^\n<console>:30: error: not found: value products_agg\n       products_agg = spark.sql(\"select productid,sum(sales) AS totalsales ,sum(refund) as totalrefund,(sum(sales)-sum(refund)) As profit from products group by productid\")\n       ^\n"}], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "val products_agg = spark.sql(\"select productid,sum(sales) AS totalsales ,sum(refund) as totalrefund,(sum(sales)-sum(refund)) As profit from products group by productid\")", "outputs": [{"output_type": "stream", "name": "stderr", "text": "org.apache.spark.sql.AnalysisException: Table or view not found: products; line 1 pos 110\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:460)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:479)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:464)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:464)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:454)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:600)\n  ... 51 elided\n"}], "metadata": {"collapsed": false}}, {"execution_count": 22, "cell_type": "code", "source": "// aggregate values\nval products_agg = spark.sql(\"select productid,sum(sales) AS totalsales ,sum(refund) as totalrefund,(sum(sales)-sum(refund)) As profit from c group by productid\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "products_agg: org.apache.spark.sql.DataFrame = [productid: int, totalsales: bigint ... 2 more fields]"}], "metadata": {"collapsed": false}}, {"execution_count": 9, "cell_type": "code", "source": "%%sql\nselect * from products_agg", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error was encountered:\norg.apache.spark.sql.AnalysisException: Table or view not found: products_agg; line 1 pos 14\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:460)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:479)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:464)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:464)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:454)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:600)\n  ... 51 elided\n"}], "metadata": {"collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": "val writeConfigMap = Map(\n\t\"Endpoint\" -> \"https://toyfactory.documents.azure.com:443/\",\n\t\"Masterkey\" -> \"uDp9gXJWbkucGznEkJBewJuiHhyJ4V1uLqle6DbsJ72egEJFL5e6MQUXzQGBZFjelJkmJPB0mFQn0haJbSlBLQ==\",\n\t\"Database\" -> \"toystore\",\n\t\"Collection\" -> \"orders_agg\", \n\t\"preferredRegions\" -> \"Central US\",\n\t\"SamplingRatio\" -> \"1.0\",\n\t\"schema_samplesize\" -> \"200000\"\n)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "writeConfigMap: scala.collection.immutable.Map[String,String] = Map(Collection -> orders_agg, Endpoint -> https://toyfactory.documents.azure.com:443/, Database -> toystore, SamplingRatio -> 1.0, schema_samplesize -> 200000, preferredRegions -> Central US, Masterkey -> uDp9gXJWbkucGznEkJBewJuiHhyJ4V1uLqle6DbsJ72egEJFL5e6MQUXzQGBZFjelJkmJPB0mFQn0haJbSlBLQ==)"}], "metadata": {"collapsed": false}}, {"execution_count": 24, "cell_type": "code", "source": "// Configuration to write\nval writeConfig = Config(writeConfigMap)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "writeConfig: com.microsoft.azure.cosmosdb.spark.config.Config = com.microsoft.azure.cosmosdb.spark.config.ConfigBuilder$$anon$1@540d70ce"}], "metadata": {"collapsed": false}}, {"execution_count": 25, "cell_type": "code", "source": "// Import SaveMode so you can Overwrite, Append, ErrorIfExists, Ignore\nimport org.apache.spark.sql.{Row, SaveMode, SparkSession}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import org.apache.spark.sql.{Row, SaveMode, SparkSession}"}], "metadata": {"collapsed": false}}, {"execution_count": 26, "cell_type": "code", "source": "// Save to Cosmos DB (using Append in this case)\nproducts_agg.write.mode(SaveMode.Overwrite).cosmosDB(writeConfig)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}