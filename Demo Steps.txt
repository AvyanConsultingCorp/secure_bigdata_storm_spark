1. Provision Cosmos DB (Use: ProvisionCosmosDB)
2. Create cosmos db database "toystore" & two collections "Orders" & "Orders_agg" (Not Yet Automated)
3. Get cosmos db connection details 
	- Endpoint
	- Master key
4. Provision Event Hub (Not Yet Automated)
5. Get Event Hub connection details	
	- Write Endpoint
	- Read policy name (Default = Reader)
	- Read policy key 
	- Event hub namespace
	- event hub name 
	- partitions 
6. Create Power BI Live dashboard and get following details (Not Yet Automated)
	- Power BI push URL 
7.1 Provision Storm Cluster (Use: ProvisionStormCluster-ScriptActions)
7.2 Provision Spark cluster (Use: ProvisionSparkCluster)
7.3 Upload Spark-To-CosmosDB\azure-cosmosdb-spark_2.1.0_2.11-0.0.4 jars to \example\jars on spark cluster default storage
8. In Storm-topology folder, provide all of the above values in dev.properties file.
9. Copy dev.properties to the storm cluster
10. Copy \target\EventHubExample-1.0-SNAPSHOT.jar to storm cluster
11. Copy dev.properties to storm cluster
12. start the storm topologies on storm cluster
13. Open spark cluster jupyter notebook dashboard and upload toystore-scala.ipynb.

Success Criteria
	- Power BI live dashboard shows live data 
	- Spark jupyter notebook successfully reads data from orders collection and inserts into orders_agg collection
	- query the cosmos db collections to verify that data is getting inserted into the collectins

	